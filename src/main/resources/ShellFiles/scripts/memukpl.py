# PipeLine —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Å—É–±—Ç–∏—Ç—Ä—ñ–≤ —ñ —É–∫—Ä–∞—ó–Ω–æ–º–æ–≤–Ω–æ—ó –æ–∑–≤—É—á–∫–∏ —Ñ–∞–π–ª—É –∑ –≤—ñ–¥–µ–æ
#
#
#

import os
import subprocess
import srt
import pysrt
import tempfile
from pydub import AudioSegment
#from TTS.api import TTS
import nltk
from ukrainian_tts.tts import TTS, Voices, Stress
import argparse

VIDEO_FILE = "foxnewsexsample.mp4"
EN_SRT = "english.srt"
UK_SRT = "ukrainian.srt"
AUDIO_FOLDER = "tts_parts"
AUDIO_FOLDER_SPEED = "tts_speed"
FINAL_AUDIO = "final_audio.wav"
FINAL_VIDEO = "final_output.mp4"


def transcribe_video():
    print(">> üéôÔ∏è –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—è –≤—ñ–¥–µ–æ...")
    subprocess.run([
        "whisper", VIDEO_FILE,
        "--language", "English",
        "--model", "medium",
        "--output_format", "srt"
    ])
    os.rename(VIDEO_FILE.replace(".mp4", ".srt"), EN_SRT)

def translate_srt_with_ollama(chunk_set):
#    print(">> üåê –ü–µ—Ä–µ–∫–ª–∞–¥ —Å—É–±—Ç–∏—Ç—Ä—ñ–≤ —á–µ—Ä–µ–∑ Ollama...")
#    with open(EN_SRT, "r", encoding="utf-8") as f:
#        srt_text = f.read()
    prompt = (
            "–ü–µ—Ä–µ–∫–ª–∞–¥–∏ —Å—É–±—Ç–∏—Ç—Ä–∏ –∑ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—ó –Ω–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É –ø–æ–≤–Ω—ñ—Å—Ç—é –±–µ–∑ —Å—Ç–∏–ª—ñ—Å—Ç–∏—á–Ω–∏—Ö —Ç–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∏—Ö –∑–º—ñ–Ω, –∑–±–µ—Ä—ñ–≥–∞—é—á–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å—É–±—Ç–∏—Ç—Ä—ñ–≤ —Ñ–æ—Ä–º–∞—Ç—É srt:\n"
#            
#            "–ü–µ—Ä–µ–∫–ª–∞–¥–∏ –Ω–∞—Å—Ç—É–ø–Ω—ñ —Å—É–±—Ç–∏—Ç—Ä–∏ –∑ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—ó –Ω–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É –ø–æ–≤–Ω—ñ—Å—Ç—é –∑–±–µ—Ä—ñ–≥–∞—é—á–∏ —Ñ–æ—Ä–º–∞—Ç .srt:\n" + srt_text
    )
    for lchunk in chunk_set:
        prompt = prompt + str(lchunk.index) + "\n"
        prompt = prompt + str(lchunk.start) + " --> "+ str(lchunk.end) +"\n"
        prompt = prompt + lchunk.text + "\n\n"
    print(prompt)
    #result = subprocess.run(["ollama", "run", "mistral"], input=prompt, text=True, capture_output=True)
    #result = subprocess.run(["ollama", "run", "llama3"], input=prompt, text=True, capture_output=True)
    result = subprocess.run(["ollama", "run", "phi4"], input=prompt, text=True, capture_output=True)
    translated_srt = result.stdout

    with open(str(chunk_set[0].index).zfill(5) +"-" + UK_SRT, "w", encoding="utf-8") as f:
        f.write(translated_srt)
    return translated_srt
#==============================================================
def is_end_of_sentence(text):
    return text.strip()[-1:] in '.!?'

def split_subtitles(subs, min_len=7, max_len=12):
    chunks = []
    chunk = []

    i = 0
    while i < len(subs):
        chunk_start = i
        chunk = []
        while i < len(subs) and len(chunk) < max_len:
            chunk.append(subs[i])
            i += 1
            if len(chunk) >= min_len and is_end_of_sentence(chunk[-1].text):
                break

        if i == len(subs) and len(chunk) < min_len and chunks:
            chunks[-1].extend(chunk)
        else:
            chunks.append(chunk)

    return [pysrt.SubRipFile(items=chunk) for chunk in chunks]

def splitSRT():
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Å—É–±—Ç–∏—Ç—Ä—ñ–≤
    subs = pysrt.open('english.srt', encoding='utf-8')

    # –û—Ç—Ä–∏–º–∞–Ω–Ω—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ñ–≤ —É –ø–∞–º'—è—Ç—ñ
    chunks_in_memory = split_subtitles(subs)
    out_translate_subtitle = ""
    for idx, chunk in enumerate(chunks_in_memory, 1):
        out_translate_subtitle = out_translate_subtitle + translate_srt_with_ollama(chunk)
    with open(UK_SRT, "w", encoding="utf-8") as f:
        f.write(out_translate_subtitle)



def synthesize_segmented_speech(voice_gender="female", speed=1.0, gain_db=0.0, model_name=None):
    print(">> üó£Ô∏è –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –æ–∑–≤—É—á–µ–Ω–Ω—è –∑ —Ç–∞–π–º—ñ–Ω–≥–∞–º–∏...")


    with open(UK_SRT, "r", encoding="utf-8") as f:
        subs = list(srt.parse(f.read()))

    nltk.download("punkt")
    from nltk.tokenize import sent_tokenize
    tts = TTS(device="cpu")

    if not os.path.exists(AUDIO_FOLDER):
        os.makedirs(AUDIO_FOLDER)
    if not os.path.exists(AUDIO_FOLDER_SPEED):
        os.makedirs(AUDIO_FOLDER_SPEED)

    for i, sub in enumerate(subs):
        text = sub.content.strip()
        duration = sub.end.total_seconds() - sub.start.total_seconds()
        filename = os.path.join(AUDIO_FOLDER, f"part_{i:04d}.wav")
        temp_path = os.path.join(AUDIO_FOLDER, f"temp_{i:04d}.wav")

        with open(temp_path, mode="wb") as file:
            _, output_text = tts.tts(text, Voices.Oleksa.value, Stress.Dictionary.value, file)
        segment = AudioSegment.from_wav(temp_path)
        segment.export(filename, format="wav")

# –∑–∞–ø–∏—Å–∞–ª–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –≤ temp_path
def prepared_speach(voice_gender="female", speed=1.0, gain_db=0.0, model_name=None):
    if not os.path.exists(AUDIO_FOLDER):
        os.makedirs(AUDIO_FOLDER)
    if not os.path.exists(AUDIO_FOLDER_SPEED):
        os.makedirs(AUDIO_FOLDER_SPEED)

    with open(UK_SRT, "r", encoding="utf-8") as f:
        subs = list(srt.parse(f.read()))

    for i, sub in enumerate(subs):
        text = sub.content.strip()
        duration = sub.end.total_seconds() - sub.start.total_seconds()
        filename = os.path.join(AUDIO_FOLDER, f"part_{i:04d}.wav")
        # temp_path = os.path.join(AUDIO_FOLDER, f"temp_{i:04d}.wav")
        filename_speed = os.path.join(AUDIO_FOLDER_SPEED, f"part_{i:04d}.wav")
        temp_path_speed = os.path.join(AUDIO_FOLDER_SPEED, f"temp_{i:04d}.wav")

        subprocess.run([
            'ffmpeg',
            '-y',
            '-i', 
            filename,
            '-filter_complex', '[0:a]atempo=1.45[a]',
            '-map', '[a]',
            filename_speed
        ])

        segment = AudioSegment.from_wav(filename_speed)

        # –ó–º—ñ–Ω—É —à–≤–∏–¥–∫–æ—Å—Ç—ñ –∑—Ä–æ–±–∏–≤ ffmpeg-–æ–º
        if speed != 1.0:
            segment = segment._spawn(segment.raw_data, overrides={
                "frame_rate": int(segment.frame_rate * speed)
            }).set_frame_rate(segment.frame_rate)

        target_duration_ms = duration * 1000 # –¢—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –∑–∞ —á–∞—Å–æ–º —Å—É–±—Ç–∏—Ç—Ä—É
        diff = target_duration_ms - len(segment) # –†—ñ–∑–Ω–∏—Ü—è –º—ñ–∂ —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—é —Å—É–±—Ç–∏—Ç—Ä—É —Ç–∞ —Å–∏–Ω—Ç–µ–∑–æ–≤–∞–Ω–∏–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–º.

        if diff > 0:
            segment += AudioSegment.silent(duration=diff) # –î–æ–¥–∞—î–º–æ —Ç–∏—à—É –¥–æ –¥–æ–≤–∂–∏–Ω–∏ —Å–µ–≥–∏–µ–Ω—Ç–∞ —É –≤–∏–ø–∞–¥–∫—É, –∫–æ–ª–∏ —Å–∏–Ω—Ç–µ–∑–æ–≤–∞–Ω–∏–π –∫–æ—Ä–æ—Ç—à–µ
        elif diff < 0:
            segment = segment[:target_duration_ms] # –æ–±—Ä—ñ–∑–∞—î–º–æ —Å–∏–Ω—Ç–µ–∑–æ–≤–∞–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç

        segment += gain_db
        segment.export(filename_speed, format="wav")
        # os.remove(temp_path)

    print(">> üîä –û–±'—î–¥–Ω–∞–Ω–Ω—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ñ–≤ –≤ –æ–¥–∏–Ω –∞—É–¥—ñ–æ—Ñ–∞–π–ª...")

    combined = AudioSegment.silent(duration=0) # combined - —Ü–µ —Ä–µ–∑—É–ª—å—Ç—É—é—á–∏–π —Ñ–∞–π–ª, —è–∫–∏–π –Ω–∞–ø–æ–≤–Ω—é—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏
    for i, sub in enumerate(subs):
        start_ms = sub.start.total_seconds() * 1000
        current_len = len(combined)
        if start_ms > current_len:
            # —è–∫—â–æ –∫—ñ–Ω–µ—Ü—å —Ä–µ–∑—É–ª—å—Ç—É—é—á–æ–≥–æ —Ñ–∞–π–ª—É –º–µ–Ω—à–µ –ø–æ —á–∞—Å—É –Ω—ñ–∂ –ø–æ—á–∞—Ç–æ–∫ - –¥–æ–¥–∞—î–º–æ —Ç–∏—à—É
            combined += AudioSegment.silent(duration=start_ms - current_len)
        # —Å—á–∏—Ç—É—î–º–æ –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–∏–π —Ñ–∞–π–ª
        part_path = os.path.join(AUDIO_FOLDER_SPEED, f"part_{i:04d}.wav")
        voice = AudioSegment.from_wav(part_path)
        combined += voice
    # –ó–∞–ø–∏—Å—É—î–º–æ —Ñ—ñ–Ω–∞–ª—å–Ω–∏–π —Ñ–∞–π–ª
    combined.export(FINAL_AUDIO, format="wav")

def combine_video_audio_subs():
    print(">> üé¨ –ö–æ–º–ø—ñ–ª—è—Ü—ñ—è —Ñ—ñ–Ω–∞–ª—å–Ω–æ–≥–æ –≤—ñ–¥–µ–æ...")
    temp_video = "temp_video.mp4"
    temp_video_edge = "temp_videoi_edge.mp4"

    subprocess.run([
        "ffmpeg", "-y", "-i", VIDEO_FILE, "-i", FINAL_AUDIO,
        "-c:v", "copy", "-map", "0:v:0", "-map", "1:a:0", "-shortest", temp_video
    ])

#        "ffmpeg", "-y", "-i", temp_video,
    subprocess.run([
        "ffmpeg", "-y", "-i", temp_video,
        "-filter_complex", f"[0:v]edgedetect=low=0.1:high=0.4", temp_video_edge
    ])


    subprocess.run([
        "ffmpeg", "-y", "-i", temp_video_edge, "-vf", f"subtitles={UK_SRT}:force_style='Borderstyle=4,Fontsize=16,BackColour=&H00000000,PrimaryColour=&H0000FFFF'", FINAL_VIDEO
    ])
#    os.remove(temp_video)


def main():
    parser = argparse.ArgumentParser(description="üé• –í—ñ–¥–µ–æ ‚Üí –°—É–±—Ç–∏—Ç—Ä–∏ ‚Üí –ü–µ—Ä–µ–∫–ª–∞–¥ ‚Üí –û–∑–≤—É—á–∫–∞ ‚Üí –í—ñ–¥–µ–æ")
    parser.add_argument("--video", type=str, default="your_video.mp4", help="–ù–∞–∑–≤–∞ –≤—ñ–¥–µ–æ—Ñ–∞–π–ª—É")
    parser.add_argument("--speed", type=float, default=1.0, help="–®–≤–∏–¥–∫—ñ—Å—Ç—å –≥–æ–ª–æ—Å—É (1.0 = –Ω–æ—Ä–º–∞–ª—å–Ω–æ)")
    parser.add_argument("--gain", type=float, default=0.0, help="–ì—É—á–Ω—ñ—Å—Ç—å —É dB (0 = —è–∫ —î)")
    parser.add_argument("--gender", type=str, default="female", help="–ì–æ–ª–æ—Å: female –∞–±–æ male")
    parser.add_argument("--model", type=str, default="tts_models/multilingual/multi-dataset/your_model", help="–ù–∞–∑–≤–∞ TTS-–º–æ–¥–µ–ª—ñ")
    args = parser.parse_args()

    global VIDEO_FILE
    VIDEO_FILE = args.video

#    transcribe_video()
#    splitSRT()
#
#    synthesize_segmented_speech(
#        voice_gender=args.gender,
#        speed=args.speed,
#        gain_db=args.gain,
#        model_name=args.model
#    )
    prepared_speach()
    combine_video_audio_subs()

    print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ! –†–µ–∑—É–ª—å—Ç–∞—Ç —É —Ñ–∞–π–ª—ñ: {FINAL_VIDEO}")


if __name__ == "__main__":
    main()
